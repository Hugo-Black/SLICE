{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hugo-Black/SLICE/blob/main/SLICE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVDhrOYKqiry"
      },
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/devanshishah1/SLICE/main/slice_logo.png\" height=\"250\" align=\"right\" style=\"height:350\">\n",
        "\n",
        "\n",
        "<h1 style=\"font-weight:800; margin-bottom:2px;\">SLICE</h1>\n",
        "\n",
        "###_Signal-Peptide Locating, Identifying, and Cleavage Engine_\n",
        "\n",
        "---\n",
        "**Authors**: James Pang and Devanshi Shah\n",
        "\n",
        "**Date**: May 30th 2025\n",
        "\n",
        "---\n",
        "\n",
        "**Usage Instructions**\n",
        "\n",
        "_Set up the following directories_\n",
        "\n",
        "**`path_to_signalp6_tar_gz`**: Directory for GPU-converted fast and slow-sequential model weights for SignalP6.0\n",
        "\n",
        "**`path_to_input`**: Directory for input fasta files, one batch = one fasta.\n",
        "\n",
        "**`path_to_output`**: Directory for results to be saved\n",
        "\n",
        "\n",
        "_Results_\n",
        "\n",
        "**`output_mode`** will determine whether mature fastas will be exported as one\n",
        "multifasta per batch or one fasta per protein.\n",
        "\n",
        "This document will output the Signal-P6.0 results, a summary .csv file and the cleaved batch in a fasta file.\n",
        "\n",
        "_Signalp6 Parameters_\n",
        "\n",
        "| Parameter               |Description                                                                                       |\n",
        "|:------------------------|:--------------------------------------------------------------------------------------------------|\n",
        "| `organism`                |  `Which organism group to use. Choices: other, eukarya`. |\n",
        "| `format`                | ` Output file format. Options: none, txt, png, eps, all`.                                   |\n",
        "| `mode`                  |` Model mode. Options: fast`, `slow-sequential`.                                                    |\n",
        "| `gpu_load`              | `Target GPU load factor (≥100 recommended to maximize VRAM usage).`                                  |\n",
        "\n",
        "\n",
        "For further information on selecting the correct parameters, please refer to:\n",
        "https://services.healthtech.dtu.dk/services/SignalP-6.0/\n",
        "\n",
        "**Acknowledgements**\n",
        "\n",
        "This tool leverages the open‐source **SignalP-6.0** framework developed by the **Nielsen Lab (DTU Bioinformatics)**, under the SignalP6.0 licensing terms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqjH54rvUE7o",
        "outputId": "32a98bcf-2983-4c15-bb89-8b67aca408a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title **Mount google drive**\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuEX5knhc9d8",
        "outputId": "d3dd6511-1b8f-4715-8ed2-36beb9b39f3e",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 FASTA file(s) in the input directory.\n",
            "FASTA file names for batch processing: ['t_cruzi.fasta']\n"
          ]
        }
      ],
      "source": [
        "#@title **Setting up Path to Model Weights, I/O Paths & Output Mode**\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define your input paths:\n",
        "path_to_signalp6_tar_gz = \"/content/drive/MyDrive/FYP/SLICE/signalp6_gpu.tar.gz\"  #@param {type:\"string\"}\n",
        "path_to_input = \"/content/drive/MyDrive/FYP/SLICE/input/\"  #@param {type:\"string\"}\n",
        "path_to_output = \"/content/drive/MyDrive/FYP/SLICE/t_cruzi_output/\"  #@param {type:\"string\"}\n",
        "output_mode = \"one multifasta per batch\"  #@param [\"one protein per fasta\", \"one multifasta per batch\"]\n",
        "\n",
        "\n",
        "if not os.path.exists(path_to_signalp6_tar_gz):\n",
        "    raise FileNotFoundError(f\"Error: File {path_to_signalp6_tar_gz} not found. Check if it's in the right directory.\")\n",
        "\n",
        "if not os.path.exists(path_to_input):\n",
        "    raise FileNotFoundError(f\"Error: Input path {path_to_input} not found. Check the path.\")\n",
        "\n",
        "if not os.path.exists(path_to_output):\n",
        "    raise FileNotFoundError(f\"Error: Output path {path_to_output} not found. Check the path.\")\n",
        "\n",
        "\n",
        "# Handle batch mode: determine whether the input is a directory or a single FASTA file.\n",
        "fasta_files = []\n",
        "if os.path.isdir(path_to_input):\n",
        "    # For batch mode, expect one multifasta file per batch.\n",
        "    # This example considers files ending with .fasta or .fa.\n",
        "    full_paths = glob.glob(os.path.join(path_to_input, \"*.fasta\")) + glob.glob(os.path.join(path_to_input, \"*.fa\"))\n",
        "    if not full_paths:\n",
        "        raise FileNotFoundError(\"No FASTA files found in the input directory.\")\n",
        "    else:\n",
        "        print(f\"Found {len(full_paths)} FASTA file(s) in the input directory.\")\n",
        "    # Get only the file names (e.g. ABC.fasta)\n",
        "    fasta_files = [os.path.basename(f) for f in full_paths]\n",
        "elif os.path.isfile(path_to_input):\n",
        "    fasta_files = [os.path.basename(path_to_input)]\n",
        "    print(\"Input is a single FASTA file.\")\n",
        "else:\n",
        "    raise ValueError(\"Input path is neither a valid file nor a directory.\")\n",
        "\n",
        "print(\"FASTA file names for batch processing:\", fasta_files)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TvSNRzKYx9v",
        "outputId": "ec198e9f-09e6-45ac-ae21-03846fcd644a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.0.2)\n",
            "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n",
            "/content\n",
            "signalp6_gpu/\n",
            "signalp6_gpu/signalp-6-package/\n",
            "signalp6_gpu/signalp-6-package/requirements.txt\n",
            "signalp6_gpu/signalp-6-package/signalp/\n",
            "signalp6_gpu/signalp-6-package/signalp/__main__.py\n",
            "signalp6_gpu/signalp-6-package/signalp/make_sequence_plot.py\n",
            "signalp6_gpu/signalp-6-package/signalp/conversion_utils/\n",
            "signalp6_gpu/signalp-6-package/signalp/conversion_utils/signalp6_convert_models\n",
            "signalp6_gpu/signalp-6-package/signalp/conversion_utils/__init__.py\n",
            "signalp6_gpu/signalp-6-package/signalp/conversion_utils/move_checkpoint_to_device.py\n",
            "signalp6_gpu/signalp-6-package/signalp/__init__.py\n",
            "signalp6_gpu/signalp-6-package/signalp/predict.py\n",
            "signalp6_gpu/signalp-6-package/signalp/sequential_model.py\n",
            "signalp6_gpu/signalp-6-package/signalp/make_output_files.py\n",
            "signalp6_gpu/signalp-6-package/signalp/model_weights/\n",
            "signalp6_gpu/signalp-6-package/signalp/model_weights/README.md\n",
            "signalp6_gpu/signalp-6-package/signalp/model_weights/distilled_model_signalp6.pt\n",
            "signalp6_gpu/signalp-6-package/signalp/utils.py\n",
            "signalp6_gpu/signalp-6-package/setup.py\n",
            "signalp6_gpu/signalp-6-package/README.md\n",
            "signalp6_gpu/signalp-6-package/signalp6.egg-info/\n",
            "signalp6_gpu/signalp-6-package/signalp6.egg-info/top_level.txt\n",
            "signalp6_gpu/signalp-6-package/signalp6.egg-info/SOURCES.txt\n",
            "signalp6_gpu/signalp-6-package/signalp6.egg-info/PKG-INFO\n",
            "signalp6_gpu/signalp-6-package/signalp6.egg-info/requires.txt\n",
            "signalp6_gpu/signalp-6-package/signalp6.egg-info/dependency_links.txt\n",
            "signalp6_gpu/signalp-6-package/signalp6.egg-info/entry_points.txt\n",
            "signalp6_gpu/signalp-6-package/build/\n",
            "signalp6_gpu/signalp-6-package/build/scripts-3.10/\n",
            "signalp6_gpu/signalp-6-package/build/scripts-3.10/signalp6_convert_models\n",
            "signalp6_gpu/signalp-6-package/build/lib/\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/__main__.py\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/make_sequence_plot.py\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/conversion_utils/\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/conversion_utils/__init__.py\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/conversion_utils/move_checkpoint_to_device.py\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/__init__.py\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/predict.py\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/sequential_model.py\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/make_output_files.py\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/model_weights/\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/model_weights/README.md\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/model_weights/distilled_model_signalp6.pt:Zone.Identifier\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/model_weights/distilled_model_signalp6.pt\n",
            "signalp6_gpu/signalp-6-package/build/lib/signalp/utils.py\n",
            "signalp6_gpu/signalp-6-package/build/bdist.linux-x86_64/\n",
            "signalp6_gpu/signalp-6-package/pyproject.toml\n",
            "signalp6_gpu/signalp-6-package/models/\n",
            "signalp6_gpu/signalp-6-package/models/README.md\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/test_0_val_2.pt:Zone.Identifier\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/test_1_val_0.pt\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/test_2_val_0.pt:Zone.Identifier\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/test_0_val_2.pt\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/test_2_val_1.pt\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/test_2_val_0.pt\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/test_0_val_1.pt:Zone.Identifier\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/averaged_viterbi.pt\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/test_1_val_2.pt\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/averaged_viterbi.pt:Zone.Identifier\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/test_0_val_1.pt\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/test_2_val_1.pt:Zone.Identifier\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/test_1_val_2.pt:Zone.Identifier\n",
            "signalp6_gpu/signalp-6-package/models/sequential_models_signalp6/test_1_val_0.pt:Zone.Identifier\n",
            "/content/signalp6_gpu/signalp-6-package\n",
            "Processing /content/signalp6_gpu/signalp-6-package\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>3.3.2 in /usr/local/lib/python3.11/dist-packages (from signalp6==6.0+h) (3.10.0)\n",
            "Requirement already satisfied: numpy>1.19.2 in /usr/local/lib/python3.11/dist-packages (from signalp6==6.0+h) (2.0.2)\n",
            "Collecting torch<2,>1.7.0 (from signalp6==6.0+h)\n",
            "  Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: tqdm>4.46.1 in /usr/local/lib/python3.11/dist-packages (from signalp6==6.0+h) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.3.2->signalp6==6.0+h) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.3.2->signalp6==6.0+h) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.3.2->signalp6==6.0+h) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.3.2->signalp6==6.0+h) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.3.2->signalp6==6.0+h) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.3.2->signalp6==6.0+h) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.3.2->signalp6==6.0+h) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.3.2->signalp6==6.0+h) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch<2,>1.7.0->signalp6==6.0+h) (4.13.2)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2,>1.7.0->signalp6==6.0+h)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2,>1.7.0->signalp6==6.0+h)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch<2,>1.7.0->signalp6==6.0+h)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2,>1.7.0->signalp6==6.0+h)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>1.7.0->signalp6==6.0+h) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>1.7.0->signalp6==6.0+h) (0.45.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>3.3.2->signalp6==6.0+h) (1.17.0)\n",
            "Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: signalp6\n",
            "  Building wheel for signalp6 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for signalp6: filename=signalp6-6.0+h-py3-none-any.whl size=1526986635 sha256=5af6f70e5901e275aea8a717b268d891035d76cdc19a9d42ed47c07328378acf\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/b4/36/1d5bee90d23c93b83c976930d4c40a1552843e26f33e3245bd\n",
            "Successfully built signalp6\n",
            "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, signalp6\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\n",
            "accelerate 1.6.0 requires torch>=2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.11.0 requires torch>=2, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 signalp6-6.0+h torch-1.13.1\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\n",
            "accelerate 1.6.0 requires torch>=2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "#@title **Setting Up Environment (~10 - 15 mins)**\n",
        "# Install dependencies\n",
        "!pip install biopython\n",
        "\n",
        "\n",
        "# Copy and extract signalP6\n",
        "!cp {path_to_signalp6_tar_gz} /content/\n",
        "%cd /content/\n",
        "!tar -xvzf /content/signalp6_gpu.tar.gz\n",
        "\n",
        "\n",
        "# Install signalP6\n",
        "%cd /content/signalp6_gpu/signalp-6-package/\n",
        "!pip install .\n",
        "\n",
        "# Fix numpy version issue\n",
        "!pip install numpy==\"1.26.4\"\n",
        "!pip install pandas==\"2.2.2\"\n",
        "\n",
        "\n",
        "# Copy model files to the appropriate directory\n",
        "SIGNALP_DIR = os.popen('python3 -c \"import signalp; import os; print(os.path.dirname(signalp.__file__))\"').read().strip()\n",
        "!cp -r /content/signalp6_gpu/signalp-6-package/models/* {SIGNALP_DIR}/model_weights/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRgf7o_-nJwV"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title **Running SignalP6.0h**\n",
        "\n",
        "#@markdown **SignalP-6.0 Parameters**\n",
        "\n",
        "import re\n",
        "organism = \"eukarya\"  #@param [\"other\", \"eukarya\"]\n",
        "format = \"none\"  #@param [\"txt\", \"png\", \"eps\", \"all\", \"none\"]\n",
        "mode = \"slow-sequential\"  #@param [\"fast\",\"slow-sequential\"]\n",
        "#@markdown **Performance Optimizations**\n",
        "gpu_load = 200  #@param {type:\"integer\"}\n",
        "#@markdown GPU Load ≥ 100 is recommended to maximise GPU VRAM usage.\n",
        "\n",
        "\n",
        "for fasta_name in fasta_files:\n",
        "    base_name = re.sub(r'\\.fasta$|\\.fa$', '', fasta_name, flags=re.IGNORECASE)\n",
        "\n",
        "    results_dir_file = os.path.join(path_to_output, f\"{base_name}_{mode}_signalp6.0_results\")\n",
        "    os.makedirs(results_dir_file, exist_ok=True)\n",
        "\n",
        "    # Build the SignalP6 command using the current FASTA file.\n",
        "    cmd = (\n",
        "        f\"signalp6 --model_dir {SIGNALP_DIR}/model_weights \"\n",
        "        f\"--fasta {os.path.join(path_to_input, fasta_name)} \"\n",
        "        f\"--organism {organism} \"\n",
        "        f\"--output_dir {results_dir_file} \"\n",
        "        f\"--format {format} \"\n",
        "        f\"--mode {mode} \"\n",
        "        f\"--bsize {gpu_load}\"\n",
        "    )\n",
        "\n",
        "#@markdown Please refer to [SignalP-6.0 Installation Instructions](https://github.com/fteufel/signalp-6.0/blob/main/installation_instructions.md) for further information on parameter options and performance optimizations.\n",
        "\n",
        "    print(f\"Running SignalP6 on {fasta_name} ...\")\n",
        "    # Execute the command.\n",
        "    !{cmd}\n",
        "    print(f\"Results for {fasta_name} are in {results_dir_file}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31vZ51c1ul5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d225e7-ff39-41d6-8d39-bd7925d2120a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing t_cruzi.fasta\n",
            "Saved summaries to /content/drive/MyDrive/FYP/SLICE/t_cruzi_output/t_cruzi_slow-sequential_summary\n",
            "569/569 files written\n",
            "All 569 FASTAs in /content/drive/MyDrive/FYP/SLICE/t_cruzi_output/t_cruzi_slow-sequential_mature_fastas\n"
          ]
        }
      ],
      "source": [
        "#@title **Batch FASTA Cleaving and Results Summary**\n",
        "\n",
        "\n",
        "# Temporary debug\n",
        "import sys, numpy as np\n",
        "try:\n",
        "    from numpy.core import records as rec\n",
        "    np.rec = rec\n",
        "    sys.modules['numpy.rec'] = rec\n",
        "except ImportError:\n",
        "    import numpy.recarray as recarray\n",
        "    np.rec = recarray\n",
        "    sys.modules['numpy.rec'] = recarray\n",
        "\n",
        "------------------------------------------------\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from Bio import SeqIO\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "\n",
        "# Function to extract the cleavage site from a \"CS Position\" string in SignalP-6.0 output.\n",
        "def extract_cleavage_site(cs_string):\n",
        "    match = re.search(r'CS pos:\\s*(\\d+)-(\\d+)', cs_string)\n",
        "    if match:\n",
        "        return int(match.group(1)), int(match.group(2))\n",
        "    return None, None\n",
        "\n",
        "# Function to extract the prediction probability from the \"CS Position\" string in SignalP-6.0 output.\n",
        "def extract_prediction_probability(cs_string):\n",
        "    match = re.search(r'Pr:\\s*([\\d\\.]+)', cs_string)\n",
        "    if match:\n",
        "        return float(match.group(1))\n",
        "    return None\n",
        "\n",
        "# Function to process each row in in SignalP-6.0 output to determine the signal peptide and mature sequence details.\n",
        "def process_row(row):\n",
        "    protein_id = row['ID'].split()[0]\n",
        "    full_seq = fasta_dict.get(protein_id)\n",
        "    full_length = len(full_seq) if full_seq is not None else None\n",
        "\n",
        "    sp_type = row.get('Prediction', None)\n",
        "\n",
        "    cs = row['CS Position']\n",
        "    cleavage_start, cleavage_end = extract_cleavage_site(cs) if pd.notnull(cs) else (None, None)\n",
        "    prediction_probability = extract_prediction_probability(cs) if pd.notnull(cs) else None\n",
        "\n",
        "    if full_seq is None:\n",
        "        ret\n",
        "            \"Protein_ID\": protein_id,\n",
        "            \"Full_Length\": None,\n",
        "            \"Signal_Peptide_Type\": sp_type,          # include type even if missing seq\n",
        "            \"Signal_Peptide\": None,\n",
        "            \"Mature_Sequence\": None,\n",
        "            \"Signal_Peptide_Pos\": None,\n",
        "            \"Mature_Sequence_Pos\": None,\n",
        "            \"Prediction_Probability\": prediction_probability\n",
        "        })\n",
        "\n",
        "    if cleavage_start is not None:\n",
        "        signal_peptide_seq = full_seq[:cleavage_start]\n",
        "        mature_seq = full_seq[cleavage_start:]\n",
        "        signal_peptide_pos = f\"1-{cleavage_start}\"\n",
        "        mature_seq_pos = f\"{cleavage_start + 1}-{full_length}\"\n",
        "    else:\n",
        "        signal_peptide_seq = \"No Signal Peptide\"\n",
        "        mature_seq = full_seq\n",
        "        signal_peptide_pos = \"None\"\n",
        "        mature_seq_pos = f\"1-{full_length}\"\n",
        "\n",
        "    return pd.Series({\n",
        "        \"Protein_ID\": protein_id,\n",
        "        \"Full_Length\": full_length,\n",
        "        \"Signal_Peptide_Type\": sp_type,           # NEW column\n",
        "        \"Signal_Peptide\": signal_peptide_seq,\n",
        "        \"Mature_Sequence\": mature_seq,\n",
        "        \"Signal_Peptide_Pos\": signal_peptide_pos,\n",
        "        \"Mature_Sequence_Pos\": mature_seq_pos,\n",
        "        \"Prediction_Probability\": prediction_probability\n",
        "    })\n",
        "\n",
        "#Cleaving and Results Summaries\n",
        "for fasta_name in fasta_files:\n",
        "    base_name = re.sub(r'\\.fasta$|\\.fa$', '', fasta_name, flags=re.IGNORECASE)\n",
        "\n",
        "    summary_dir = os.path.join(path_to_output, f\"{base_name}_{mode}_summaries\")\n",
        "    results_dir = os.path.join(path_to_output, f\"{base_name}_{mode}_results\")\n",
        "    prediction_file = os.path.join(results_dir, \"prediction_results.txt\")\n",
        "\n",
        "    if not os.path.exists(prediction_file):\n",
        "        print(f\"Warning: no predictions for {fasta_name}, skipping.\")\n",
        "        continue\n",
        "    print(f\"\\nProcessing {fasta_name}\")\n",
        "\n",
        "    df = pd.read_csv(prediction_file, sep='\\t', skiprows=1)\n",
        "    df.columns = df.columns.str.lstrip('# ').str.strip()\n",
        "\n",
        "    # load input FASTA into dictionary\n",
        "    fasta_path = None\n",
        "    for ext in ('.fasta', '.fa'):\n",
        "        p = os.path.join(path_to_input, base_name + ext)\n",
        "        if os.path.exists(p):\n",
        "            fasta_path = p\n",
        "            break\n",
        "    if fasta_path is None:\n",
        "        print(f\"Warning: no FASTA for {base_name}, skipping.\")\n",
        "        continue\n",
        "    fasta_dict = {r.id: str(r.seq) for r in SeqIO.parse(fasta_path, \"fasta\")}\n",
        "\n",
        "    # apply process_row and create full summary DF\n",
        "    summary_df = df.apply(process_row, axis=1)\n",
        "\n",
        "    # filtered DF of only secreted proteins\n",
        "    summary_df_signal = summary_df[\n",
        "        summary_df['Signal_Peptide_Type'].isin(['SP','LIPO','TAT','TATLIPO','PILIN'])\n",
        "    ].reset_index(drop=True)\n",
        "\n",
        "    # save full & filtered summaries\n",
        "    os.makedirs(summary_dir, exist_ok=True)\n",
        "    summary_df.to_csv(os.path.join(summary_dir, f\"{base_name}_{mode}_summary.csv\"), index=False)\n",
        "    summary_df_signal.to_csv(os.path.join(summary_dir, f\"{base_name}_{mode}_SP_hits_summary.csv\"), index=False)\n",
        "    print(f\"Saved summaries to {summary_dir}\")\n",
        "\n",
        "    # build mature FASTA\n",
        "    records = []\n",
        "    for _, row in summary_df_signal.iterrows():\n",
        "        seq = row['Mature_Sequence']\n",
        "        if not seq:\n",
        "            continue\n",
        "        records.append(SeqRecord(Seq(seq), id=row['Protein_ID'], description=\"\"))\n",
        "\n",
        "    mature_fastas_dir = os.path.join(path_to_output, f\"{base_name}_{mode}_mature_fastas\")\n",
        "    os.makedirs(mature_fastas_dir, exist_ok=True)\n",
        "\n",
        "    if output_mode == \"one multifasta per batch\":\n",
        "        out = os.path.join(mature_fastas_dir, f\"{base_name}_{mode}_full.fasta\")\n",
        "        SeqIO.write(records, out, \"fasta\")\n",
        "        print(f\"Wrote {len(records)} sequences to {out}\")\n",
        "\n",
        "    elif output_mode == \"one protein per fasta\":\n",
        "        total = len(records)\n",
        "        for i, rec in enumerate(records, 1):\n",
        "            fn = os.path.join(mature_fastas_dir, f\"{rec.id}_cleaved.fasta\")\n",
        "            SeqIO.write([rec], fn, \"fasta\")\n",
        "            if i % 1000 == 0 or i == total:\n",
        "                print(f\"{i}/{total} files written\")\n",
        "        print(f\"All {total} FASTAs in {mature_fastas_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}